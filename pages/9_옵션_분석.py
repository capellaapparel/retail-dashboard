# ==========================================
# File: pages/9_ì˜µì…˜_ë¶„ì„.py
# (Altairë§Œ ì‚¬ìš© Â· ë„ë„› ìœ„ì— ì¹´í…Œê³ ë¦¬ í…ìŠ¤íŠ¸ í‘œê¸°)
# ==========================================
import streamlit as st
import pandas as pd
import numpy as np
import altair as alt
import re
from dateutil import parser

st.set_page_config(page_title="ì˜µì…˜ Â· ì¹´í…Œê³ ë¦¬ ë¶„ì„", layout="wide")
st.title("ğŸ§© ì˜µì…˜ Â· ì¹´í…Œê³ ë¦¬ ë¶„ì„")

# -------------------------
# Helpers
# -------------------------
@st.cache_data(show_spinner=False)
def load_google_sheet(sheet_name: str) -> pd.DataFrame:
    import gspread
    from oauth2client.service_account import ServiceAccountCredentials
    import json
    GOOGLE_SHEET_URL = "https://docs.google.com/spreadsheets/d/1oyVzCgGK1Q3Qi_sbYwE-wKG6SArnfUDRe7rQfGOF-Eo"
    scope = ["https://spreadsheets.google.com/feeds","https://www.googleapis.com/auth/drive"]
    creds_json = {k: str(v) for k, v in st.secrets["gcp_service_account"].items()}
    with open("/tmp/service_account.json","w") as f: json.dump(creds_json, f)
    creds = ServiceAccountCredentials.from_json_keyfile_name("/tmp/service_account.json", scope)
    client = gspread.authorize(creds)
    ws = client.open_by_url(GOOGLE_SHEET_URL).worksheet(sheet_name)
    df = pd.DataFrame(ws.get_all_records())
    df.columns = [c.lower().strip() for c in df.columns]
    return df

STYLE_RE = re.compile(r"\b([A-Z]{1,3}\d{3,5}[A-Z0-9]?)\b")

def parse_temudate(x):
    s = str(x)
    if "(" in s: s = s.split("(")[0].strip()
    try: return parser.parse(s, fuzzy=True)
    except Exception: return pd.NaT

def parse_sheindate(x):
    try: return pd.to_datetime(str(x), errors="coerce", infer_datetime_format=True)
    except Exception: return pd.NaT

def style_key_from_label(label: str, img_map: dict) -> str | None:
    s = str(label).strip().upper()
    if not s: return None
    s_key = s.replace(" ", "")
    if s_key in img_map: return s_key
    m = STYLE_RE.search(s)
    if m:
        cand = m.group(1).replace(" ", "")
        if cand in img_map: return cand
    for k in img_map.keys():
        if k in s_key: return k
    return None

def normalize_size(x: str) -> str:
    s = str(x).strip().upper().replace(" ", "")
    mapping = {
        "1XL": "1X", "2XL": "2X", "3XL": "3X",
        "SMALL":"S", "MEDIUM":"M", "LARGE":"L"
    }
    return mapping.get(s, s)

def parse_shein_sku(sku: str):
    """
    SHEIN 'seller sku' ì˜ˆ) ABC123-HEATHER_GREY-1X
    ìƒ‰ìƒì€ _ ë¥¼ ê³µë°±ìœ¼ë¡œ ë°”ê¿” í‘œê¸°
    """
    s = str(sku)
    if "-" not in s:
        return None, None, None
    parts = s.split("-")
    if len(parts) < 3:
        return None, None, None
    style = parts[0]
    color = parts[1].replace("_", " ").title()
    size  = normalize_size(parts[2])
    return style, color, size

# ì¹´í…Œê³ ë¦¬ íŒì • ë¡œì§
TOP_TOKENS   = {"CROP TOP", "WAIST TOP", "LONG TOP"}
DRESS_TOKENS = {"MINI DRESS","MIDI DRESS","MAXI DRESS"}
SKIRT_TOKENS = {"MINI SKIRT","MIDI SKIRT","MAXI SKIRT"}

def infer_category(length_str: str, temu_name: str) -> str:
    name = str(temu_name).upper()
    if "ROMPER" in name:    return "ROMPER"
    if "JUMPSUIT" in name:  return "JUMPSUIT"

    tokens = [t.strip().upper() for t in str(length_str).split(",") if t.strip()]
    has_top   = any(t in TOP_TOKENS   for t in tokens)
    has_dress = any(t in DRESS_TOKENS for t in tokens)
    has_skirt = any(t in SKIRT_TOKENS for t in tokens)

    # ì„¸íŠ¸: TOP + (SKIRT ë˜ëŠ” PANTSë¥˜) ì¡°í•©
    if has_top and (has_skirt or ("PANTS" in str(length_str).upper())):
        return "SET"
    if has_top:   return "TOP"
    if has_dress: return "DRESS"
    if has_skirt: return "SKIRT"
    # ê¸°ë³¸ê°’ì€ PANTS (OTHER ì œê±° ìš”ì²­ ë°˜ì˜)
    return "PANTS"

# -------------------------
# Load
# -------------------------
info  = load_google_sheet("PRODUCT_INFO")
temu  = load_google_sheet("TEMU_SALES")
shein = load_google_sheet("SHEIN_SALES")

IMG_MAP = dict(zip(
    info.get("product number", pd.Series(dtype=str)).astype(str).str.upper().str.replace(" ", "", regex=False),
    info.get("image","")
))

temu["order date"]  = temu["purchase date"].apply(parse_temudate)
shein["order date"] = shein["order processed on"].apply(parse_sheindate)

# ë‚ ì§œ ì»¨íŠ¸ë¡¤
min_dt = pd.to_datetime(pd.concat([temu["order date"], shein["order date"]]).dropna()).min()
max_dt = pd.to_datetime(pd.concat([temu["order date"], shein["order date"]]).dropna()).max()

left, right = st.columns([1.3, 1])
with left:
    dr = st.date_input(
        "ì¡°íšŒ ê¸°ê°„",
        value=(max_dt.date() - pd.Timedelta(days=29), max_dt.date()),
        min_value=min_dt.date(),
        max_value=max_dt.date(),
    )
    if isinstance(dr, (list, tuple)):
        start, end = dr
    else:
        start, end = dr, dr
    start = pd.to_datetime(start)
    end   = pd.to_datetime(end) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)
with right:
    platform = st.radio("í”Œë«í¼", ["BOTH","TEMU","SHEIN"], horizontal=True)

# -------------------------
# íŒë§¤ ë°ì´í„°(ìƒ‰ìƒÂ·ì‚¬ì´ì¦ˆ í¬í•¨) êµ¬ì¶•
# -------------------------
# TEMU: shipped/delivered & qty ì‚¬ìš©
t = temu[(temu["order date"]>=start) & (temu["order date"]<=end)]
t = t[t["order item status"].astype(str).str.lower().isin(["shipped","delivered"])].copy()
t["qty"]   = pd.to_numeric(t.get("quantity shipped", 0), errors="coerce").fillna(0)
t["style"] = t["product number"].astype(str)
t["color"] = t.get("color", "")
t["size"]  = t.get("size", "")

# SHEIN: refunded ì œì™¸, ì£¼ë¬¸ í•œ ê±´ = 1 qty
s = shein[(shein["order date"]>=start) & (shein["order date"]<=end)]
s = s[~s["order status"].astype(str).str.lower().eq("customer refunded")].copy()
style_, color_, size_ = [], [], []
for sku in s.get("seller sku", pd.Series([""]*len(s))):
    sty, col, siz = parse_shein_sku(sku)
    style_.append(sty); color_.append(col); size_.append(siz)
s["style"] = style_
s["color"] = color_
s["size"]  = size_
s["qty"]   = 1.0

if platform == "TEMU":
    sold = t.copy()
elif platform == "SHEIN":
    sold = s.copy()
else:
    sold = pd.concat([t, s], ignore_index=True)

# ì‚¬ì´ì¦ˆ í‘œì¤€í™”
sold["size"] = sold["size"].apply(normalize_size)

# -------------------------
# ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (ìŠ¤íƒ€ì¼ë³„)
# -------------------------
# TEMU ìƒí’ˆëª… í…ìŠ¤íŠ¸ í™•ë³´í•´ ì¹´í…Œê³ ë¦¬ ë³´ì •ì— ì‚¬ìš©
temu_name_by_style = (
    temu.groupby(temu["product number"].astype(str))["product name by customer order"]
        .agg(lambda x: next((v for v in x if str(v).strip()), ""))
        .to_dict()
)

cat_map = {}
for _, r in info.iterrows():
    key = str(r.get("product number", "")).upper().replace(" ", "")
    length_str = str(r.get("length",""))
    temu_name = temu_name_by_style.get(str(r.get("product number","")), "")
    cat_map[key] = infer_category(length_str, temu_name)

# soldì— ì¹´í…Œê³ ë¦¬ ë¶€ì—¬
def stykey(x: str) -> str:
    return str(x).upper().replace(" ", "")

sold["style_key"] = sold["style"].astype(str).apply(stykey)
sold["cat"] = sold["style_key"].map(cat_map).fillna("PANTS")

# -------------------------
# 1) ë„ë„›: ì¹´í…Œê³ ë¦¬ë³„ íŒë§¤ ë¹„ìœ¨ (í‘ë°± ì¶œë ¥ ëŒ€ë¹„ ë¼ë²¨ ì§ì ‘ í‘œê¸°)
# -------------------------
cat_summary = (sold.groupby("cat")["qty"].sum().reset_index()
                    .rename(columns={"qty":"cnt"})
               )
if cat_summary["cnt"].sum() == 0:
    st.info("í•´ë‹¹ ê¸°ê°„ì— íŒë§¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

cat_summary["pct"] = cat_summary["cnt"] / cat_summary["cnt"].sum()
cat_summary = cat_summary.sort_values("cnt", ascending=False)

# Altair ë„ë„›
arc = alt.Chart(cat_summary).mark_arc(innerRadius=90, outerRadius=160).encode(
    theta=alt.Theta("cnt:Q", stack=True),
    color=alt.Color("cat:N", legend=None),  # í”„ë¦°íŠ¸(í‘ë°±)ìš©ìœ¼ë¡œ ë²”ë¡€ ìˆ¨ê¹€
    order=alt.Order("cnt:Q", sort="descending")
)

# ë¼ë²¨(ì¹´í…Œê³ ë¦¬ëª… + %). í‘ë°± í”„ë¦°íŠ¸ ëŒ€ë¹„ ê²€ì€ ê¸€ì”¨ë¡œ í¬ê²Œ
cat_summary["label"] = cat_summary.apply(
    lambda r: f"{r['cat']} ({r['pct']*100:.1f}%)", axis=1
)
labels = alt.Chart(cat_summary).mark_text(radius=190, size=14, color="black").encode(
    theta=alt.Theta("cnt:Q", stack=True),
    text="label:N",
    order=alt.Order("cnt:Q", sort="descending")
)

left_block = st.container()
with left_block:
    st.markdown("### ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ íŒë§¤ ë¹„ìœ¨ (ë„ë„›)")
    st.altair_chart((arc + labels).properties(height=420), use_container_width=True)

# ì˜¤ë¥¸ìª½ì— ê°„ë‹¨ ìš”ì•½ í…Œì´ë¸”
right_block = st.container()
with right_block:
    st.markdown("### ğŸ—‚ï¸ ì¹´í…Œê³ ë¦¬ ìš”ì•½")
    show = cat_summary[["cat","cnt","pct"]].copy()
    show["ë¹„ìœ¨(%)"] = (show["pct"]*100).round(1)
    show = show.drop(columns=["pct"]).rename(columns={"cat":"ì¹´í…Œê³ ë¦¬","cnt":"íŒë§¤ìˆ˜ëŸ‰"})
    st.dataframe(show, use_container_width=True, hide_index=True)

st.markdown("---")

# -------------------------
# 2) ì˜µì…˜ ìš”ì•½ (ìƒ‰ìƒ Top, ì‚¬ì´ì¦ˆ Top) â€” ì²˜ìŒ ìŠ¤íƒ€ì¼ ëŠë‚Œìœ¼ë¡œ ë‹¨ìˆœíˆ Topë§Œ
# -------------------------
st.subheader("ğŸ¨ ì˜µì…˜ ìš”ì•½ (ìƒ‰ìƒ/ì‚¬ì´ì¦ˆ Top)")

colA, colB = st.columns(2)

# ìƒ‰ìƒ Top
with colA:
    color_top = (sold.dropna(subset=["color"])
                     .groupby("color")["qty"].sum()
                     .sort_values(ascending=False)
                     .head(12).reset_index())
    color_chart = alt.Chart(color_top).mark_bar().encode(
        y=alt.Y("color:N", sort='-x', title="ìƒ‰ìƒ"),
        x=alt.X("qty:Q", title="íŒë§¤ìˆ˜ëŸ‰"),
        tooltip=["color","qty"]
    ).properties(height=360)
    st.altair_chart(color_chart, use_container_width=True)

# ì‚¬ì´ì¦ˆ Top
with colB:
    size_top = (sold.dropna(subset=["size"])
                    .groupby("size")["qty"].sum()
                    .sort_values(ascending=False)
                    .reset_index())
    size_chart = alt.Chart(size_top).mark_bar().encode(
        y=alt.Y("size:N", sort='-x', title="ì‚¬ì´ì¦ˆ"),
        x=alt.X("qty:Q", title="íŒë§¤ìˆ˜ëŸ‰"),
        tooltip=["size","qty"]
    ).properties(height=360)
    st.altair_chart(size_chart, use_container_width=True)

st.caption("Â· ë„ë„›ì€ 'íŒë§¤ìˆ˜ëŸ‰' ê¸°ì¤€ ë¹„ìœ¨ì…ë‹ˆë‹¤. (ìƒ‰ìƒ/ì‚¬ì´ì¦ˆëŠ” Topë§Œ í‘œì‹œ)")
